<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction and Guidelines &mdash; try_docs v1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=92fd9be5" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=2b4fdbf1" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=5cb08e4e"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="_static/copybutton.js?v=f281be69"></script>
        <script src="_static/my_custom.js?v=679418e8"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Get Started" href="Page_3.html" />
    <link rel="prev" title="TNLearn: Task-based Neurons for learning" href="README_Page_1.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html">
            
              <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="README_Page_1.html">TNLearn: Task-based Neurons for learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#motivation">Motivation</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#features">Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#benchmarks">Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#resource">Resource</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#dependences">Dependences</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#install">Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#quick-start">Quick start</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#api-documentation">API documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#citation">Citation</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#the-team">The Team</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#license">License</a></li>
<li class="toctree-l2"><a class="reference internal" href="README_Page_1.html#what-s-next">What’s Next?</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction and Guidelines</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#why-task-based-neurons">Why Task-based Neurons?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-are-task-based-neurons">What Are Task-based Neurons?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vectorized-symbolic-regression">1. Vectorized Symbolic Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parameterization">2. Parameterization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#when-to-use-task-based-neurons">When to use Task-based Neurons?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#whats-next">What’s Next?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Page_3.html">Get Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="Page_3.html#packages-required">Packages Required</a></li>
<li class="toctree-l2"><a class="reference internal" href="Page_3.html#basic-install-by-pip">Basic Install by pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="Page_3.html#build-shared-libraries-from-source">Build Shared Libraries from Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="Page_3.html#what-s-next">What’s Next</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Page_4.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="Page_4.html#tnlearn-vecsymregressor">tnlearn.VecSymRegressor</a></li>
<li class="toctree-l2"><a class="reference internal" href="Page_4.html#tnlearn-mlpclassifier">tnlearn.MLPClassifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="Page_4.html#tnlearn-mlpregressor">tnlearn.MLPRegressor</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">try_docs</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction and Guidelines</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/Page_2.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction-and-guidelines">
<h1>Introduction and Guidelines<a class="headerlink" href="#introduction-and-guidelines" title="Link to this heading"></a></h1>
<section id="why-task-based-neurons">
<h2>Why Task-based Neurons?<a class="headerlink" href="#why-task-based-neurons" title="Link to this heading"></a></h2>
<p>In the past decade, a majority of deep learning research is on designing outstanding architectures, such as the bottleneck in autoencoders <a class="footnote-reference brackets" href="#id23" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, shortcuts <a class="footnote-reference brackets" href="#id24" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id25" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, and neural architecture search (NAS) <a class="footnote-reference brackets" href="#id26" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>. Almost exclusively, these works employ neurons of the same type that use an inner product and a nonlinear activation. We refer to such a neuron as a linear neuron, and a network made of these neurons as a linear network (LN) hereafter. Recently, the field “NeuroAI” emerged <a class="footnote-reference brackets" href="#id27" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> to advocate that a large amount of neuroscience knowledge can help catalyze the next generation of AI. This idea is well-motivated, as the brain remains the most intelligent system to date, and an artificial network can be regarded as a miniature of the brain. Following the advocacy of “NeuroAI”, it is noted that our brain is made up of many functionally and morphologically different neurons, while the existing mainstream artificial networks are homogeneous at the neuronal level. Thus, why not introduce neuronal diversity into artificial networks and examine associated merits?</p>
<p>Our overarching opinion is that the neuron type and architecture are two complementary dimensions of an artificial network. Designing well-performing neurons represents a new dimension relative to designing well-performing architectures. Therefore, the neuronal type should be given full attention to harness the full potential of connectionism. In recent years, a plethora of studies have introduced new neurons into deep learning <a class="footnote-reference brackets" href="#id28" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id29" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id30" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id31" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id32" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id33" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a> such as polynomial neurons <a class="footnote-reference brackets" href="#id28" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> and quadratic neurons <a class="footnote-reference brackets" href="#id29" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id30" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id31" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id32" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id33" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>. Despite focusing only on a specific type of neuron, this thread of studies reasonably verifies the feasibility and potential of developing deep learning with new neurons.</p>
<p>Biological neuronal diversity, both in terms of morphology and functionality, arises from the brain’s needs to perform complex tasks <a class="footnote-reference brackets" href="#id34" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>. The brain does not rely on a single type of neuron to universally function in all aspects. Instead, it acts as a sophisticated designer of task-based neurons. Hence, in the realm of deep learning, we think that promoting neuronal diversity should also not be limited to specific neuron types like linear or quadratic neurons. Instead, it should take into account the specific context of the tasks at hand. Can we design different neurons for different tasks (task-based neurons)? Computationally, the philosophy of task-based architectures and task-based neurons is quite distinct.</p>
<p>The former is “one-for-all”, which implicitly assumes that stacking a universal and basic type of neurons into different structures can solve a wide class of complicated nonlinear problems. This philosophy is well underpinned by the universal approximation theorem <a class="footnote-reference brackets" href="#id35" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>. The latter is “one-for-one”, which assumes that there are no one-size-fits-all neuron types, and it is better to solve a specific problem by prototyping customized neurons. Because task-based neurons are imparted with the implicit bias for the task, the network of task-based neurons can integrate the task-driven forces of all these neurons, which given the same structure should exhibit stronger performance than the network of generic neurons. The key difference between task-based neurons and preset neurons is that the mathematical expression in the task-based neurons is adaptive according to the preference of the task, while in the preset neurons, the mathematical expression is preset.</p>
</section>
<section id="what-are-task-based-neurons">
<h2>What Are Task-based Neurons?<a class="headerlink" href="#what-are-task-based-neurons" title="Link to this heading"></a></h2>
<section id="vectorized-symbolic-regression">
<h3>1. Vectorized Symbolic Regression<a class="headerlink" href="#vectorized-symbolic-regression" title="Link to this heading"></a></h3>
<p>Unlike traditional regression algorithms that fit numerical coefficients, symbolic regression first encodes a formula into a tree structure and then uses a genetic algorithm to explore the space of possible mathematical expressions to identify the best formula. Because no gradients with respect to the mathematical formula can be computed, the most common technique for solving symbolic regression problems is genetic programming (GP) <a class="footnote-reference brackets" href="#id36" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>. GP is a powerful population-based evolutionary algorithm, which mainly uses crossover and mutation to generate new formulas. The following two figures illustrate the schematic diagrams of crossover and mutation respectively.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/Figure_crossover.png"><img alt="_images/Figure_crossover.png" src="_images/Figure_crossover.png" style="width: 528.9px; height: 217.79999999999998px;" /></a>
</figure>
<p><strong>Crossover</strong> is a genetic programming operation to generate new individuals by means of subtree crossover among the selected individuals, and then explore the symbolic expression space. The specific method is to randomly select subtrees of the winner candidates and exchange them. This operation promotes diversity in the population and can lead to the discovery of new and more effective mathematical formulas.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/Figure_mutation.png"><img alt="_images/Figure_mutation.png" src="_images/Figure_mutation.png" style="width: 561.6px; height: 212.1px;" /></a>
</figure>
<p><strong>Mutation</strong> is a genetic programming operation to randomly select a position of an individual, and generate a new individual through single-point mutation. Due to the randomness of mutation, it can re-join some functions and variables that were eliminated before, thereby potentially leading to the discovery of novel and effective expressions. By injecting variability into the population, mutation plays a crucial role in exploring the solution space and preventing premature convergence to suboptimal solutions.</p>
<p>In prototyping task-based neurons, we consider three important aspects:</p>
<ul class="simple">
<li><p>How to efficiently design task-based neurons?</p></li>
<li><p>How to make the resultant neurons transferable to a network?</p></li>
<li><p>How to transfer the superiority at the neuronal level to the network level?</p></li>
</ul>
<p>We find that the traditional symbolic regression cannot fulfill these needs, particularly for high-dimensional inputs, due to three problems: First, the regression process of traditional symbolic regression becomes slow and computationally expensive for high-dimensional inputs. The search space becomes vast for high-dimensional inputs, as it requires checking an arbitrary form of interactions among two or more input variables. Second, the formulas learned by the traditional symbolic regression are heterogeneous, which suffers from the parametric explosion for high-dimensional inputs and does not support parallel computing and GPU acceleration. Thus, such formulas cannot serve as the aggregation function of a neuron because the resultant neuron cannot be easily integrated into deep networks. Third, the traditional symbolic regression may learn overly complex formulas, subjected to the risk of overfitting when connecting those neurons into a network.</p>
<p>To address these problems, we propose a solution called vectorized symbolic regression. This approach regularizes every variable to learn the same formula, allowing us to organize all variables into a vector. The formulas are then based on vector computation, as illustrated in the following Figure. Unlike traditional symbolic regression, which tends to identify a heterogeneous formula. The vectorized symbolic regression is simple yet mighty, which has valuable characteristics suitable to this task:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/Figure_vectorized.png"><img alt="_images/Figure_vectorized.png" src="_images/Figure_vectorized.png" style="width: 440.40000000000003px; height: 296.8px;" /></a>
</figure>
<p><strong>Regression Speed</strong>: The vectorized symbolic regression decreases the computational complexity of the regression process, making it much faster than traditional symbolic regression, especially for high-dimensional inputs. This is because the search space is significantly reduced when all variables are regularized to learn the same formula.</p>
<p><strong>Low Complexity and Parallel Computing</strong>: Due to the homogeneity, the proposed vectorized symbolic regression leads to mathematical formulas with much fewer parameters. Given d-dimensional inputs, the number of parameters is O(d), which is at the same level as the linear neuron. Moreover, because each variable conducts the same operation, formulas obtained from the proposed vectorized symbolic regression can be organized into the vector or matrix computation, which can facilitate parallel computation aided by GPUs. The low complexity and parallel computing allow for faster and more efficient training of deep networks composed of task-based neurons.</p>
<p><strong>Generalization</strong>: The proposed vectorized symbolic regression has a significantly restricted search space. It is unlikely that a homogeneous formula can perfectly fit or overfit data all the time. Therefore, the learned formula tends to underfit data. The power of a neural network is not solely determined by neurons. We can introduce additional flexibility and adaptability to the network structure, enabling it to better handle complex problems and achieve the optimal generalization performance.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One may ask since linear neurons can already represent any function based on universal approximation <a class="footnote-reference brackets" href="#id37" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a>, why are task-based neurons necessary? While it is true that there is no task that can only be done by task-based neurons but not by linear neurons, the key issue is effectiveness and efficiency. It was reported that a linear network needs an exponential number of parameters to learn the multiplication operation <a class="footnote-reference brackets" href="#id39" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a>. Task-based neurons search the suitable formulas from a broad function space, which can automatically integrate task-related priors, thereby leveraging the specific strengths of these neurons to tackle complex tasks effectively. Furthermore, task-based neurons can be optimized for a specific task, which can improve the efficiency of the network.</p>
</div>
</section>
<section id="parameterization">
<h3>2. Parameterization<a class="headerlink" href="#parameterization" title="Link to this heading"></a></h3>
<p>We expect that the vectorized symbolic regression can identify hidden patterns behind data collected from different tasks. Leveraging these patterns to prototype new neurons would be useful. These patterns are basic and not necessarily specific functions. For instance, we refer to a cell as circular that is characterized by an elliptical equation, but we don’t need to specify the radius of the circle. To take advantage of these patterns, we reparameterize the learned formula by making the fixed constants trainable. Such neurons will perform better than preset neurons since considering the complexity of tasks, there should be no one-size-fits-all neurons. By reparameterizing the learned formula, we can fine-tune the neuron’s behavior to better fit the task at hand. As mentioned earlier, the task-based neurons established through vectorized symbolic regression have limited expressive ability and cannot effectively scale to handle complex tasks on their own. Given a network, the trainable parameters allow for a more efficient and effective search for the optimal solution.</p>
</section>
</section>
<section id="when-to-use-task-based-neurons">
<h2>When to use Task-based Neurons?<a class="headerlink" href="#when-to-use-task-based-neurons" title="Link to this heading"></a></h2>
<p>Task-based neurons and universal neurons are two different approaches in the field of artificial neural networks. We summarize some potential advantages of task-based neurons over universal neurons, although the advantages of task-based neurons over universal neurons can depend on the specific task and the design of the neural network:</p>
<ul class="simple">
<li><p><strong>Efficiency</strong>: Task-based neurons are designed to perform specific tasks, which can make them more efficient in terms of computational resources and time.</p></li>
<li><p><strong>Specialization</strong>: Task-based neurons can be specialized for specific tasks, which can lead to better performance in those tasks.</p></li>
<li><p><strong>Interpretability</strong>: Task-based neurons can be easier to interpret, as their function is directly related to the task they are designed for.</p></li>
</ul>
</section>
<section id="whats-next">
<h2>What’s Next?<a class="headerlink" href="#whats-next" title="Link to this heading"></a></h2>
<p>Please read the next page <a class="reference internal" href="Page_3.html"><span class="doc">Get Started</span></a> to install <code class="docutils literal notranslate"><span class="pre">tnlearn</span></code> quickly.</p>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Link to this heading"></a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id23" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas, “<a class="reference external" href="https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28">U-net: Convolutional networks for biomedical image segmentation</a>”, in <em>MICCAI</em>, pp. 234-241, Springer, 2015.</p>
</aside>
<aside class="footnote brackets" id="id24" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian, “<a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Deep residual learning for image recognition</a>”, in <em>CVPR</em>, pp. 770-778, Springer, 2016.</p>
</aside>
<aside class="footnote brackets" id="id25" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Fan, Fenglei and Wang, Dayang and Guo, Hengtao and Zhu, Qikui and Yan, Pingkun and Wang, Ge and Yu, Hengyong, “<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9614997">On a sparse shortcut topology of artificial neural networks</a>”, in <em>IEEE Transactions on Artificial Intelligence</em>, IEEE, 2021.</p>
</aside>
<aside class="footnote brackets" id="id26" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>Yang, Chengrun and Bender, Gabriel and Liu, Hanxiao and Kindermans, Pieter-Jan and Udell, Madeleine and Lu, Yifeng and Le, Quoc V and Huang, Da, “<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/4e392aa9bc70ed731d3c9c32810f92fb-Abstract-Conference.html">TabNAS: Rejection Sampling for Neural Architecture Search on Tabular Datasets</a>”, in <em>Advances in Neural Information Processing Systems</em>, pp. 11906-11917, 2022.</p>
</aside>
<aside class="footnote brackets" id="id27" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Zador, Anthony and Richards, Blake and Olveczky, Bence and Escola, Sean and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and others, “<a class="reference external" href="https://arxiv.org/abs/2210.08340">Toward next-generation artificial intelligence: Catalyzing the neuroai revolution</a>”, in <em>arXiv preprint arXiv:2210.08340</em>, 2022.</p>
</aside>
<aside class="footnote brackets" id="id28" role="note">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id6">1</a>,<a role="doc-backlink" href="#id12">2</a>)</span>
<p>Chrysos, Grigoris and Moschoglou, Stylianos and Bouritsas, Giorgos and Deng, Jiankang and Panagakis, Yannis and Zafeiriou, Stefanos P, “<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9353253">Deep Polynomial Neural Networks</a>”, in <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2021.</p>
</aside>
<aside class="footnote brackets" id="id29" role="note">
<span class="label"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id13">2</a>)</span>
<p>Fan, Feng-Lei and Li, Yingxin and Peng, Hanchuan and Zeng, Tieyong and Wang, Fei, “<a class="reference external" href="https://arxiv.org/abs/2301.09245">Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks</a>”, in <em>arXiv preprint arXiv:2301.09245</em>, 2023.</p>
</aside>
<aside class="footnote brackets" id="id30" role="note">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Jiang, Yiyang and Yang, Fan and Zhu, Hengliang and Zhou, Dian and Zeng, Xuan, “<a class="reference external" href="https://link.springer.com/article/10.1007/s00521-019-04316-4">Nonlinear CNN: improving CNNs with quadratic convolutions</a>”, in <em>Neural Computing and Applications</em>, pp. 8507-8516, Springer, 2020.</p>
</aside>
<aside class="footnote brackets" id="id31" role="note">
<span class="label"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id15">2</a>)</span>
<p>Mantini, Pranav and Shah, Shishr K, “<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9413207">Cqnn: Convolutional quadratic neural networks</a>”, in <em>2020 25th International Conference on Pattern Recognition (ICPR)</em>, pp. 9819-9826, IEEE, 2021.</p>
</aside>
<aside class="footnote brackets" id="id32" role="note">
<span class="label"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id10">1</a>,<a role="doc-backlink" href="#id16">2</a>)</span>
<p>Goyal, Mohit and Goyal, Rajan and Lall, Brejesh, “<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9207535">Improved polynomial neural networks with normalised activations</a>”, in <em>2020 International Joint Conference on Neural Networks (IJCNN)</em>, pp. 1-8, IEEE, 2020.</p>
</aside>
<aside class="footnote brackets" id="id33" role="note">
<span class="label"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id17">2</a>)</span>
<p>Liao, Jing-Xiao and Dong, Hang-Cheng and Sun, Zhi-Qi and Sun, Jinwei and Zhang, Shiping and Fan, Feng-Lei, “<a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/10076833">Attention-embedded quadratic network (qttention) for effective and interpretable bearing fault diagnosis</a>”, in <em>IEEE Transactions on Instrumentation and Measurement</em>, pp. 1-13, IEEE, 2023.</p>
</aside>
<aside class="footnote brackets" id="id34" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">12</a><span class="fn-bracket">]</span></span>
<p>Peng, Hanchuan and Xie, Peng and Liu, Lijuan and Kuang, Xiuli and Wang, Yimin and Qu, Lei and Gong, Hui and Jiang, Shengdian and Li, Anan and Ruan, Zongcai and others, “<a class="reference external" href="https://www.nature.com/articles/s41586-021-03941-1">Morphological diversity of single neurons in molecularly defined cell types</a>”, in <em>Nature</em>, pp. 174-181, Nature Publishing Group, 2021.</p>
</aside>
<aside class="footnote brackets" id="id35" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">13</a><span class="fn-bracket">]</span></span>
<p>Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert, “<a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/0893608090900056">Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks</a>”, in <em>Neural Networks</em>, pp. 551-560, Elsevier, 1990.</p>
</aside>
<aside class="footnote brackets" id="id36" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">14</a><span class="fn-bracket">]</span></span>
<p>Cramer, Nichael Lynn, “<a class="reference external" href="https://dl.acm.org/doi/10.5555/645511.657085">A representation for the adaptive generation of simple sequential programs</a>”, in <em>Proceedings of the First International Conference on Genetic Algorithms and Their Applications</em>, pp. 183-187, Psychology Press, 2014.</p>
</aside>
<aside class="footnote brackets" id="id37" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">15</a><span class="fn-bracket">]</span></span>
<p>Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert, “<a class="reference external" href="https://www.semanticscholar.org/paper/Universal-approximation-of-an-unknown-mapping-and-Hornik-Stinchcombe/37807e97c624fb846df7e559553b32539ba2ea5d">Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks</a>”, in <em>Neural Networks</em>, pp. 551-560, Elsevier, 1990.</p>
</aside>
<aside class="footnote brackets" id="id39" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">16</a><span class="fn-bracket">]</span></span>
<p>Yarotsky, Dmitry, “<a class="reference external" href="https://arxiv.org/pdf/1610.01145.pdf">A representation for the adaptive generation of simple sequential programs</a>”, in <em>Neural Networks</em>, pp. 103-114, Elsevier, 2017.</p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="README_Page_1.html" class="btn btn-neutral float-left" title="TNLearn: Task-based Neurons for learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Page_3.html" class="btn btn-neutral float-right" title="Get Started" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Finn.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>